{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c410f7e-2ca4-4f0e-a6c5-4ffffe61e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "dataset_path = r'C:\\Users\\ACER\\Downloads\\pascal_voc_dataset'\n",
    "voc2012_annotations = os.path.join(dataset_path, 'VOCdevkit', 'VOC2012', 'Annotations')\n",
    "voc2012_images = os.path.join(dataset_path, 'VOCdevkit', 'VOC2012', 'JPEGImages')\n",
    "train_dir = os.path.join(dataset_path, 'train')\n",
    "val_dir = os.path.join(dataset_path, 'val')\n",
    "\n",
    "# Organize images into train and validation directories\n",
    "def organize_images(annotation_dir, image_dir, output_dir):\n",
    "    for file in os.listdir(annotation_dir):\n",
    "        if file.endswith('.xml'):\n",
    "            tree = ET.parse(os.path.join(annotation_dir, file))\n",
    "            root = tree.getroot()\n",
    "            image_filename = root.find('filename').text\n",
    "            for obj in root.findall('object'):\n",
    "                class_name = obj.find('name').text\n",
    "                class_dir = os.path.join(output_dir, class_name)\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                image_path = os.path.join(image_dir, image_filename)\n",
    "                if os.path.exists(image_path):\n",
    "                    shutil.copy(image_path, class_dir)\n",
    "\n",
    "# Organize train and validation images\n",
    "organize_images(voc2012_annotations, voc2012_images, train_dir)\n",
    "organize_images(voc2012_annotations, voc2012_images, val_dir)\n",
    "\n",
    "# Data Generator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(64, 64), batch_size=128, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=(64, 64), batch_size=128, class_mode='categorical')\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential([\n",
    "    Input(shape=(64, 64, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(20, activation='softmax') \n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "lr_schedule = LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (epoch / 20))\n",
    "\n",
    "# Train Model with EarlyStopping and 5 epochs\n",
    "history = model.fit(train_generator, epochs=5, validation_data=val_generator, callbacks=[early_stopping, lr_schedule])\n",
    "\n",
    "# Evaluate Model\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Save Model\n",
    "model.save('optimized_pascal_voc_cnn_model.keras')\n",
    "\n",
    "# Real-Time Detection with Webcam\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('optimized_pascal_voc_cnn_model.keras')\n",
    "\n",
    "# Class labels for Pascal VOC dataset\n",
    "class_labels = [\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
    "    'bus', 'car', 'cat', 'chair', 'cow',\n",
    "    'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "frame_count = 0\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()  # Background subtraction for motion detection\n",
    "threshold = 0.2  # Reduced confidence threshold to increase detection\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Apply background subtraction to get the foreground mask\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Find contours to check for motion or objects\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Skip frame if no contours are found\n",
    "    if contours:\n",
    "        # Preprocess the frame for prediction\n",
    "        img = cv2.resize(frame, (64, 64))\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = img / 255.0\n",
    "\n",
    "        # Get predictions from the model\n",
    "        predictions = model.predict(img, verbose=0)\n",
    "\n",
    "        # Print predictions for debugging\n",
    "        print(predictions)\n",
    "\n",
    "        # Only classify if the confidence is above the threshold\n",
    "        if np.max(predictions) > threshold:\n",
    "            class_idx = np.argmax(predictions, axis=1)[0]\n",
    "            label = class_labels[class_idx]\n",
    "        else:\n",
    "            label = \"No object detected\"  # Show if confidence is below threshold\n",
    "\n",
    "        # Display the label on the frame\n",
    "        cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the frame with the label\n",
    "    cv2.namedWindow('Webcam', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Webcam', 640, 480)\n",
    "    cv2.imshow('Webcam', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
